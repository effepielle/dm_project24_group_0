{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KExTp0Dh79qQ"
      },
      "source": [
        "# **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ivhXde3e79qR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score, confusion_matrix, get_scorer, make_scorer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "sys.path.append('..')\n",
        "from utils import sensitivity_score, specificity_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-Nb5-R79qS"
      },
      "source": [
        "### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvtxKixW79qS"
      },
      "source": [
        "Import training and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-L5j9qWR79qS"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/dataset/train_data.csv')\n",
        "train_labels = pd.read_csv('/content/drive/MyDrive/dataset/train_labels.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/dataset/test_data.csv')\n",
        "test_labels = pd.read_csv('/content/drive/MyDrive/dataset/test_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9OFqlp9z_dyo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, _, train_labels, _ = train_test_split(\n",
        "    train_data, train_labels, test_size=0.8, stratify=train_labels, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMBNuguG79qT",
        "outputId": "c5f272ce-a463-423f-89b5-6c7908678432"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['_url', 'name', 'points', 'length', 'climb_total', 'profile',\n",
              "       'startlist_quality', 'position', 'cyclist', 'cyclist_age', 'is_tarmac',\n",
              "       'cyclist_team', 'duration', 'cyclist_number', 'cyclist_level',\n",
              "       'cyclist_experience', 'relative_position', 'avg_relative_position',\n",
              "       'cyclist_experience_profile', 'avg_rel_position_profile', 'length_cat',\n",
              "       'cyclist_experience_length', 'avg_rel_position_length', 'climb_cat',\n",
              "       'relative_position_sum', 'cyclist_experience_climb',\n",
              "       'avg_rel_position_climb', 'avg_cyclist_level', 'position_entropy',\n",
              "       'top_20_entropy'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z6iz2Gp79qT"
      },
      "source": [
        "Define the categorical and numeric features. SVM requires only numeric features. Standardize the numeric features to ensure all variables have the same importance in distance calculations, as SVM is sensitive to differences in scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "c6h40c9y79qT"
      },
      "outputs": [],
      "source": [
        "features_to_keep = [ 'cyclist_age', 'cyclist_number', 'cyclist_level', 'avg_relative_position',\n",
        "       'avg_rel_position_profile', 'climb_total', 'cyclist_experience_climb',\n",
        "       'avg_rel_position_climb', 'top_20_entropy']\n",
        "\n",
        "train_data = train_data[features_to_keep]\n",
        "test_data = test_data[features_to_keep]\n",
        "\n",
        "numeric_transformer = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0TtZUYpL79qU"
      },
      "outputs": [],
      "source": [
        "# Create the preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[ ('num', numeric_transformer, features_to_keep)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytELsXri79qU"
      },
      "source": [
        "## **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuXCZiBH79qU",
        "outputId": "27ec9cbf-d9be-4e06-8d74-6c4f2d253621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, classification_report\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# define the classifier\n",
        "clf = SVC(class_weight='balanced', probability=False, random_state=42)\n",
        "\n",
        "# create the pipeline\n",
        "model = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", preprocessor),  \n",
        "        (\"sampler\", RandomUnderSampler(random_state=42, sampling_strategy=0.5)),  \n",
        "        (\"svm\", clf)                     \n",
        "    ]\n",
        ")\n",
        "\n",
        "# svm hyperparameters\n",
        "parameters = {\n",
        "    'svm__C': [0.01, 0.05, 0.1, 1.0, 3.0, 5.0, 10.0],  # regularization parameter\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # kernel\n",
        "    'svm__gamma': ['scale', 'auto', 0.1, 1],  \n",
        "    'svm__degree': [2, 3, 5],  # only for poly kernel\n",
        "}\n",
        "\n",
        "\n",
        "scoring = {\n",
        "    'sensitivity': make_scorer(sensitivity_score),\n",
        "    'specificity': make_scorer(specificity_score),\n",
        "    'accuracy': get_scorer(\"accuracy\"),\n",
        "    'precision': get_scorer(\"precision\"),\n",
        "    'sensitivity': get_scorer(\"recall\"),\n",
        "    'roc_auc': get_scorer(\"roc_auc\"),\n",
        "    'f1': get_scorer(\"f1\")\n",
        "}\n",
        "\n",
        "\n",
        "rscv = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=parameters,\n",
        "    scoring=scoring,  \n",
        "    n_iter=20,  \n",
        "    cv=5,  \n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,  \n",
        "    refit=\"f1\"  \n",
        ")\n",
        "\n",
        "\n",
        "rscv.fit(train_data, train_labels.values.ravel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SK6SRgI79qV"
      },
      "source": [
        "## Model Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMCxGei379qV"
      },
      "outputs": [],
      "source": [
        "# extract the results of the randomized search and best model idx\n",
        "cv_results = rscv.cv_results_\n",
        "best_model = rscv.best_estimator_\n",
        "best_index = rscv.best_index_\n",
        "\n",
        "#extract and print matrics\n",
        "mean_test_scores = {metric: cv_results[f'mean_test_{metric}'][best_index] for metric in scoring.keys()}\n",
        "std_test_scores = {metric: cv_results[f'std_test_{metric}'][best_index] for metric in scoring.keys()}\n",
        "\n",
        "print(\"Validation results of the best model:\")\n",
        "for metric in scoring.keys():\n",
        "    print(f\"{metric.capitalize()} - Mean: {mean_test_scores[metric]:.4f}, Std: {std_test_scores[metric]:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyBousiV79qV"
      },
      "source": [
        "### Test scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO0dwM9q79qV"
      },
      "outputs": [],
      "source": [
        "# Predict on the test data\n",
        "test_predictions = rscv.predict(test_data)\n",
        "\n",
        "# Calculate and visualize evaluation metrics\n",
        "accuracy = round(accuracy_score(test_labels, test_predictions), 3)\n",
        "recall = round(recall_with_zero_division(test_labels, test_predictions), 3)\n",
        "precision = round(precision_with_zero_division(test_labels, test_predictions), 3)\n",
        "sensitivity_score = round(sensitivity_score(test_labels, test_predictions), 3)\n",
        "specificity_score = round(specificity_score(test_labels, test_predictions), 3)\n",
        "f1 = round(f1_score(test_labels, test_predictions), 3)\n",
        "roc_auc = round(roc_auc_score(test_labels, test_predictions), 3)\n",
        "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Sensitivity: {sensitivity_score}\")\n",
        "print(f\"Specificity: {specificity_score}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"ROC AUC Score: {roc_auc}\")\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MWFZG9O79qV"
      },
      "outputs": [],
      "source": [
        "report = classification_report(test_labels, test_predictions)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn30mOf979qW"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('best_models', exist_ok=True)\n",
        "# Save the best model\n",
        "joblib.dump(best_model, 'best_models/rule_based.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
